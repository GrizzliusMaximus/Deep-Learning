{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0308 20:27:23.545459 139763093591872 deprecation.py:506] From /home/chico/.virtualenvs/dl4cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 256)       7168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                81930     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 3,629,066\n",
      "Trainable params: 3,629,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 1.6838 - acc: 0.3870 - val_loss: 1.3524 - val_acc: 0.5251\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 1.3249 - acc: 0.5281 - val_loss: 1.1518 - val_acc: 0.5974\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 1.1691 - acc: 0.5884 - val_loss: 1.0005 - val_acc: 0.6509\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 1.0618 - acc: 0.6279 - val_loss: 0.9047 - val_acc: 0.6886\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.9845 - acc: 0.6559 - val_loss: 0.8236 - val_acc: 0.7182\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.9264 - acc: 0.6765 - val_loss: 0.7812 - val_acc: 0.7334\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.8787 - acc: 0.6942 - val_loss: 0.7429 - val_acc: 0.7424\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.8412 - acc: 0.7081 - val_loss: 0.7140 - val_acc: 0.7524\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.8037 - acc: 0.7213 - val_loss: 0.7217 - val_acc: 0.7484\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.7754 - acc: 0.7324 - val_loss: 0.6920 - val_acc: 0.7612\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.7516 - acc: 0.7397 - val_loss: 0.6455 - val_acc: 0.7792\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 0.7294 - acc: 0.7483 - val_loss: 0.6559 - val_acc: 0.7794\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.7087 - acc: 0.7546 - val_loss: 0.6010 - val_acc: 0.7922\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.6833 - acc: 0.7641 - val_loss: 0.6069 - val_acc: 0.7921\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.6675 - acc: 0.7684 - val_loss: 0.5964 - val_acc: 0.7972\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 40s 40ms/step - loss: 0.6525 - acc: 0.7745 - val_loss: 0.5617 - val_acc: 0.8057\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 40s 40ms/step - loss: 0.6379 - acc: 0.7800 - val_loss: 0.5658 - val_acc: 0.8036\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6203 - acc: 0.7854 - val_loss: 0.5816 - val_acc: 0.7985\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6077 - acc: 0.7892 - val_loss: 0.5663 - val_acc: 0.8062\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 63s 63ms/step - loss: 0.5965 - acc: 0.7942 - val_loss: 0.5443 - val_acc: 0.8157\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5856 - acc: 0.7979 - val_loss: 0.5270 - val_acc: 0.8202\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.5736 - acc: 0.8011 - val_loss: 0.5313 - val_acc: 0.8179\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.5611 - acc: 0.8058 - val_loss: 0.5205 - val_acc: 0.8250\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.5514 - acc: 0.8090 - val_loss: 0.5042 - val_acc: 0.8270\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.5429 - acc: 0.8120 - val_loss: 0.4838 - val_acc: 0.8348\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 62s 62ms/step - loss: 0.5328 - acc: 0.8154 - val_loss: 0.4869 - val_acc: 0.8332\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 62s 62ms/step - loss: 0.5237 - acc: 0.8181 - val_loss: 0.4713 - val_acc: 0.8385\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5150 - acc: 0.8216 - val_loss: 0.4864 - val_acc: 0.8364\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5047 - acc: 0.8254 - val_loss: 0.4872 - val_acc: 0.8334\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4965 - acc: 0.8276 - val_loss: 0.4840 - val_acc: 0.8352\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4895 - acc: 0.8300 - val_loss: 0.4577 - val_acc: 0.8469\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4833 - acc: 0.8331 - val_loss: 0.4555 - val_acc: 0.8449\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4767 - acc: 0.8347 - val_loss: 0.4522 - val_acc: 0.8464\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4687 - acc: 0.8370 - val_loss: 0.4698 - val_acc: 0.8397\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4567 - acc: 0.8412 - val_loss: 0.4589 - val_acc: 0.8475\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4523 - acc: 0.8433 - val_loss: 0.4611 - val_acc: 0.8425\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4480 - acc: 0.8441 - val_loss: 0.4295 - val_acc: 0.8513\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4410 - acc: 0.8458 - val_loss: 0.4407 - val_acc: 0.8507\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4367 - acc: 0.8481 - val_loss: 0.4403 - val_acc: 0.8505\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4281 - acc: 0.8516 - val_loss: 0.4265 - val_acc: 0.8571\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4254 - acc: 0.8522 - val_loss: 0.4506 - val_acc: 0.8493\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4183 - acc: 0.8544 - val_loss: 0.4187 - val_acc: 0.8585\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4135 - acc: 0.8554 - val_loss: 0.4170 - val_acc: 0.8614\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4056 - acc: 0.8579 - val_loss: 0.4509 - val_acc: 0.8485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.4032 - acc: 0.8600 - val_loss: 0.4225 - val_acc: 0.8577\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3975 - acc: 0.8619 - val_loss: 0.4137 - val_acc: 0.8593\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.3925 - acc: 0.8629 - val_loss: 0.4179 - val_acc: 0.8569\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3870 - acc: 0.8643 - val_loss: 0.4139 - val_acc: 0.8611\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3828 - acc: 0.8666 - val_loss: 0.3995 - val_acc: 0.8659\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3763 - acc: 0.8690 - val_loss: 0.3972 - val_acc: 0.8665\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.3740 - acc: 0.8696 - val_loss: 0.4029 - val_acc: 0.8651\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 63s 63ms/step - loss: 0.3722 - acc: 0.8692 - val_loss: 0.4288 - val_acc: 0.8586\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3676 - acc: 0.8713 - val_loss: 0.3983 - val_acc: 0.8669\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3590 - acc: 0.8752 - val_loss: 0.3938 - val_acc: 0.8692\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3574 - acc: 0.8759 - val_loss: 0.3992 - val_acc: 0.8663\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3561 - acc: 0.8764 - val_loss: 0.3897 - val_acc: 0.8683\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3511 - acc: 0.8767 - val_loss: 0.4138 - val_acc: 0.8639\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.3432 - acc: 0.8797 - val_loss: 0.4137 - val_acc: 0.8618\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.3418 - acc: 0.8802 - val_loss: 0.4054 - val_acc: 0.8646\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.3381 - acc: 0.8812 - val_loss: 0.3943 - val_acc: 0.8687\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 40s 40ms/step - loss: 0.3332 - acc: 0.8838 - val_loss: 0.3961 - val_acc: 0.8696\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3303 - acc: 0.8842 - val_loss: 0.3811 - val_acc: 0.8710\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.3262 - acc: 0.8860 - val_loss: 0.3859 - val_acc: 0.8710\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 47s 47ms/step - loss: 0.3221 - acc: 0.8865 - val_loss: 0.4067 - val_acc: 0.8657\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 62s 62ms/step - loss: 0.3229 - acc: 0.8866 - val_loss: 0.4007 - val_acc: 0.8654\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.3209 - acc: 0.8876 - val_loss: 0.3910 - val_acc: 0.8690\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3143 - acc: 0.8883 - val_loss: 0.3832 - val_acc: 0.8750\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.3103 - acc: 0.8910 - val_loss: 0.3755 - val_acc: 0.8756\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.3083 - acc: 0.8919 - val_loss: 0.3897 - val_acc: 0.8715\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 40s 40ms/step - loss: 0.3045 - acc: 0.8926 - val_loss: 0.3881 - val_acc: 0.8736\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3019 - acc: 0.8939 - val_loss: 0.3596 - val_acc: 0.8796\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3005 - acc: 0.8945 - val_loss: 0.3822 - val_acc: 0.8738\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.2960 - acc: 0.8967 - val_loss: 0.3744 - val_acc: 0.8763\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 63s 63ms/step - loss: 0.2925 - acc: 0.8971 - val_loss: 0.3662 - val_acc: 0.8794\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.2875 - acc: 0.8989 - val_loss: 0.3813 - val_acc: 0.8769\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2861 - acc: 0.8992 - val_loss: 0.3908 - val_acc: 0.8733\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2860 - acc: 0.8986 - val_loss: 0.3777 - val_acc: 0.8741\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2815 - acc: 0.9007 - val_loss: 0.3725 - val_acc: 0.8758\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2793 - acc: 0.9011 - val_loss: 0.3671 - val_acc: 0.8815\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 62s 62ms/step - loss: 0.2769 - acc: 0.9029 - val_loss: 0.3775 - val_acc: 0.8769\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2770 - acc: 0.9022 - val_loss: 0.3641 - val_acc: 0.8804\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2745 - acc: 0.9027 - val_loss: 0.3720 - val_acc: 0.8784\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 64s 64ms/step - loss: 0.2687 - acc: 0.9057 - val_loss: 0.3674 - val_acc: 0.8790\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.2712 - acc: 0.9046 - val_loss: 0.3737 - val_acc: 0.8775\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2660 - acc: 0.9068 - val_loss: 0.3595 - val_acc: 0.8805\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2631 - acc: 0.9064 - val_loss: 0.3596 - val_acc: 0.8820\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2606 - acc: 0.9080 - val_loss: 0.3814 - val_acc: 0.8774\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2592 - acc: 0.9080 - val_loss: 0.3671 - val_acc: 0.8780\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 40s 40ms/step - loss: 0.2573 - acc: 0.9093 - val_loss: 0.3763 - val_acc: 0.8798\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2562 - acc: 0.9089 - val_loss: 0.3679 - val_acc: 0.8782\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 0.2507 - acc: 0.9110 - val_loss: 0.3603 - val_acc: 0.8833\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.2505 - acc: 0.9115 - val_loss: 0.3572 - val_acc: 0.8818\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 40s 40ms/step - loss: 0.2493 - acc: 0.9122 - val_loss: 0.3628 - val_acc: 0.8807\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2455 - acc: 0.9125 - val_loss: 0.3653 - val_acc: 0.8796\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2443 - acc: 0.9141 - val_loss: 0.3664 - val_acc: 0.8842\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2421 - acc: 0.9146 - val_loss: 0.3532 - val_acc: 0.8850\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2399 - acc: 0.9148 - val_loss: 0.3530 - val_acc: 0.8839\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2380 - acc: 0.9155 - val_loss: 0.3619 - val_acc: 0.8826\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2356 - acc: 0.9165 - val_loss: 0.3608 - val_acc: 0.8850\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2346 - acc: 0.9172 - val_loss: 0.3759 - val_acc: 0.8802\n",
      "10000/10000 [==============================] - 1s 144us/step\n",
      "Test loss: 0.37592223373651507\n",
      "Test accuracy: 0.8802\n",
      "Time taken in min:  74.66129029591879\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CNN on CIFAR10 dataset\n",
    "\n",
    "With Augmentation:\n",
    "    76% Validation Accuracy @Epoch 15\n",
    "    78% Validation Accuracy @Epoch 20\n",
    "    80% Validation Accuracy @Epoch 25\n",
    "    84% Validation Accuracy @Epoch 50\n",
    "    87% Validation Accuracy @Epoch 100\n",
    "\n",
    "    38ms/step on an RTX 2070\n",
    "    64 minutes and 20 sec to complete 100 Epochs\n",
    "    1000 steps per Epoch\n",
    "\n",
    "Without Augmentation:\n",
    "    75% Validation Accuracy @Epoch 15\n",
    "    78% Validation Accuracy @Epoch 25\n",
    "    81% Validation Accuracy @Epoch 50\n",
    "    83% Validation Accuracy @Epoch 100\n",
    "    \n",
    "    ~318us/step on an RTX 2070\n",
    "    26 minutes and 21 seconds to complete 100 Epochs\n",
    "    50000 steps per Epoch\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical, plot_model, Sequence\n",
    "from keras import backend as K\n",
    "import os\n",
    "\n",
    "if K.backend() == 'tensorflow': #prevents out CUDA out of memory\n",
    "    import tensorflow as tf\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "stime = time.time()\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "batch_size = 128 \n",
    "#WARNING: This depth setup (256->512->512) utilizes 90% of the GPU using an RTX 2070  \n",
    "depth1 = 256 \n",
    "depth2 = 512\n",
    "depth3 = 512\n",
    "hidden_units = 1024\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "num_predictions = 20\n",
    "image_size = x_train.shape[1]\n",
    "input_shape = (image_size, image_size, 3)\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(depth1, (3, 3), padding='same', activation = 'relu', input_shape=x_train.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(depth2, (3, 3), padding='same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(depth2, (3, 3), padding='same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "plot_model(model, to_file='cnn-mnist.png', show_shapes=True)\n",
    "\n",
    "\n",
    "opt = keras.optimizers.adam(lr=0.0001, decay=1e-6)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "data_augmentation = True\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    datagen = ImageDataGenerator(\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        width_shift_range=0.1, # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1, # randomly shift images vertically (fraction of total height)\n",
    "        fill_mode='nearest',\n",
    "        horizontal_flip=True  # randomly flip images\n",
    "        )\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "        batch_size=batch_size),\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        use_multiprocessing = False,\n",
    "        workers=12, #number of cores\n",
    "        steps_per_epoch = 1000)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "etime = time.time()\n",
    "print('Time taken in min: ', (etime-stime)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
