{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0308 19:02:29.063536 140044213495616 deprecation.py:506] From /home/chico/.virtualenvs/dl4cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 256)       7168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                81930     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 3,629,066\n",
      "Trainable params: 3,629,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chico/.virtualenvs/dl4cv/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 40s 40ms/step - loss: 1.6830 - acc: 0.3808 - val_loss: 1.4285 - val_acc: 0.4819\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 1.3277 - acc: 0.5251 - val_loss: 1.2370 - val_acc: 0.5677\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 36s 36ms/step - loss: 1.1253 - acc: 0.6014 - val_loss: 1.0889 - val_acc: 0.6172\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 1.0266 - acc: 0.6416 - val_loss: 0.9853 - val_acc: 0.6589\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.9173 - acc: 0.6813 - val_loss: 0.8890 - val_acc: 0.6909\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.8460 - acc: 0.7039 - val_loss: 0.8835 - val_acc: 0.6934\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.8208 - acc: 0.7140 - val_loss: 0.9020 - val_acc: 0.6971\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.7745 - acc: 0.7319 - val_loss: 0.7765 - val_acc: 0.7313\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.7452 - acc: 0.7424 - val_loss: 0.8353 - val_acc: 0.7223\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.7162 - acc: 0.7532 - val_loss: 0.7983 - val_acc: 0.7292\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6864 - acc: 0.7668 - val_loss: 0.7479 - val_acc: 0.7449\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6589 - acc: 0.7715 - val_loss: 0.7108 - val_acc: 0.7605\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6394 - acc: 0.7796 - val_loss: 0.7357 - val_acc: 0.7510\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6256 - acc: 0.7837 - val_loss: 0.6825 - val_acc: 0.7679\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5883 - acc: 0.7968 - val_loss: 0.7165 - val_acc: 0.7626\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5635 - acc: 0.8072 - val_loss: 0.6840 - val_acc: 0.7751\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.5797 - acc: 0.8017 - val_loss: 0.6523 - val_acc: 0.7817\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.5675 - acc: 0.8049 - val_loss: 0.6391 - val_acc: 0.7862\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5440 - acc: 0.8132 - val_loss: 0.6291 - val_acc: 0.7905\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5148 - acc: 0.8212 - val_loss: 0.6322 - val_acc: 0.7886\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5224 - acc: 0.8219 - val_loss: 0.6415 - val_acc: 0.7853\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5194 - acc: 0.8212 - val_loss: 0.6088 - val_acc: 0.7956\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5018 - acc: 0.8280 - val_loss: 0.5924 - val_acc: 0.8041\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4882 - acc: 0.8333 - val_loss: 0.6031 - val_acc: 0.8016\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4870 - acc: 0.8344 - val_loss: 0.6345 - val_acc: 0.7872\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4985 - acc: 0.8302 - val_loss: 0.6342 - val_acc: 0.7902\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4965 - acc: 0.8274 - val_loss: 0.5937 - val_acc: 0.8035\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4855 - acc: 0.8314 - val_loss: 0.5732 - val_acc: 0.8086\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4444 - acc: 0.8471 - val_loss: 0.5839 - val_acc: 0.8102\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4449 - acc: 0.8478 - val_loss: 0.5679 - val_acc: 0.8158\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4391 - acc: 0.8490 - val_loss: 0.5609 - val_acc: 0.8154\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4435 - acc: 0.8480 - val_loss: 0.5528 - val_acc: 0.8218\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4346 - acc: 0.8501 - val_loss: 0.5404 - val_acc: 0.8186\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4122 - acc: 0.8566 - val_loss: 0.5378 - val_acc: 0.8222\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4190 - acc: 0.8571 - val_loss: 0.5210 - val_acc: 0.8277\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4095 - acc: 0.8595 - val_loss: 0.5459 - val_acc: 0.8232\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4203 - acc: 0.8556 - val_loss: 0.5172 - val_acc: 0.8329\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3810 - acc: 0.8696 - val_loss: 0.5624 - val_acc: 0.8180\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3907 - acc: 0.8653 - val_loss: 0.5328 - val_acc: 0.8225\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3910 - acc: 0.8655 - val_loss: 0.4914 - val_acc: 0.8365\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3807 - acc: 0.8687 - val_loss: 0.5309 - val_acc: 0.8260\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3835 - acc: 0.8679 - val_loss: 0.5109 - val_acc: 0.8337\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3501 - acc: 0.8799 - val_loss: 0.5209 - val_acc: 0.8351\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3439 - acc: 0.8821 - val_loss: 0.5062 - val_acc: 0.8330\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3648 - acc: 0.8741 - val_loss: 0.5436 - val_acc: 0.8327\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3579 - acc: 0.8755 - val_loss: 0.5130 - val_acc: 0.8332\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3450 - acc: 0.8819 - val_loss: 0.5241 - val_acc: 0.8331\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3347 - acc: 0.8843 - val_loss: 0.4919 - val_acc: 0.8409\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3380 - acc: 0.8844 - val_loss: 0.4725 - val_acc: 0.8453\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3341 - acc: 0.8836 - val_loss: 0.4846 - val_acc: 0.8469\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3369 - acc: 0.8825 - val_loss: 0.4958 - val_acc: 0.8457\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3395 - acc: 0.8834 - val_loss: 0.4823 - val_acc: 0.8470\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3105 - acc: 0.8930 - val_loss: 0.4885 - val_acc: 0.8467\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3151 - acc: 0.8912 - val_loss: 0.4893 - val_acc: 0.8423\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3071 - acc: 0.8921 - val_loss: 0.4892 - val_acc: 0.8433\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3173 - acc: 0.8911 - val_loss: 0.4706 - val_acc: 0.8473\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3071 - acc: 0.8947 - val_loss: 0.4788 - val_acc: 0.8480\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3004 - acc: 0.8954 - val_loss: 0.4747 - val_acc: 0.8497\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3025 - acc: 0.8955 - val_loss: 0.4769 - val_acc: 0.8493\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3008 - acc: 0.8950 - val_loss: 0.4875 - val_acc: 0.8492\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3003 - acc: 0.8964 - val_loss: 0.4693 - val_acc: 0.8568\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2732 - acc: 0.9066 - val_loss: 0.4535 - val_acc: 0.8557\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2816 - acc: 0.9011 - val_loss: 0.4786 - val_acc: 0.8528\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2882 - acc: 0.9006 - val_loss: 0.4810 - val_acc: 0.8506\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2902 - acc: 0.8996 - val_loss: 0.4844 - val_acc: 0.8459\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2886 - acc: 0.8997 - val_loss: 0.4609 - val_acc: 0.8550\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2720 - acc: 0.9061 - val_loss: 0.4777 - val_acc: 0.8497\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2737 - acc: 0.9048 - val_loss: 0.4946 - val_acc: 0.8515\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2749 - acc: 0.9037 - val_loss: 0.4567 - val_acc: 0.8588\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2688 - acc: 0.9057 - val_loss: 0.4467 - val_acc: 0.8602\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2692 - acc: 0.9061 - val_loss: 0.4518 - val_acc: 0.8648\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2564 - acc: 0.9108 - val_loss: 0.4628 - val_acc: 0.8597\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2670 - acc: 0.9072 - val_loss: 0.4404 - val_acc: 0.8684\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2624 - acc: 0.9089 - val_loss: 0.4422 - val_acc: 0.8622\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2680 - acc: 0.9068 - val_loss: 0.4392 - val_acc: 0.8613\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2419 - acc: 0.9162 - val_loss: 0.4602 - val_acc: 0.8579\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2426 - acc: 0.9164 - val_loss: 0.4424 - val_acc: 0.8625\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2534 - acc: 0.9115 - val_loss: 0.4462 - val_acc: 0.8618\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2557 - acc: 0.9111 - val_loss: 0.4528 - val_acc: 0.8609\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2466 - acc: 0.9152 - val_loss: 0.4403 - val_acc: 0.8687\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2301 - acc: 0.9207 - val_loss: 0.4491 - val_acc: 0.8627\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2327 - acc: 0.9188 - val_loss: 0.4406 - val_acc: 0.8660\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2398 - acc: 0.9170 - val_loss: 0.4270 - val_acc: 0.8703\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2548 - acc: 0.9113 - val_loss: 0.4390 - val_acc: 0.8650\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2416 - acc: 0.9152 - val_loss: 0.4365 - val_acc: 0.8671\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2276 - acc: 0.9203 - val_loss: 0.4498 - val_acc: 0.8664\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2293 - acc: 0.9200 - val_loss: 0.4514 - val_acc: 0.8657\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2285 - acc: 0.9203 - val_loss: 0.4423 - val_acc: 0.8705\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2373 - acc: 0.9188 - val_loss: 0.4603 - val_acc: 0.8634\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2112 - acc: 0.9263 - val_loss: 0.4203 - val_acc: 0.8708\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2179 - acc: 0.9243 - val_loss: 0.4341 - val_acc: 0.8694\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2180 - acc: 0.9235 - val_loss: 0.4258 - val_acc: 0.8740\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2283 - acc: 0.9207 - val_loss: 0.4189 - val_acc: 0.8719\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2366 - acc: 0.9172 - val_loss: 0.4278 - val_acc: 0.8714\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2073 - acc: 0.9275 - val_loss: 0.4566 - val_acc: 0.8630\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2154 - acc: 0.9243 - val_loss: 0.4261 - val_acc: 0.8722\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2134 - acc: 0.9259 - val_loss: 0.4348 - val_acc: 0.8708\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2173 - acc: 0.9251 - val_loss: 0.4216 - val_acc: 0.8683\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2051 - acc: 0.9284 - val_loss: 0.4458 - val_acc: 0.8714\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.1949 - acc: 0.9315 - val_loss: 0.4028 - val_acc: 0.8740\n",
      "10000/10000 [==============================] - 1s 141us/step\n",
      "Test loss: 0.40283156876564025\n",
      "Test accuracy: 0.874\n",
      "Time taken in min:  63.46148364941279\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CNN on CIFAR10 dataset\n",
    "\n",
    "With Augmentation:\n",
    "    76% Validation Accuracy @Epoch 15\n",
    "    78% Validation Accuracy @Epoch 20\n",
    "    80% Validation Accuracy @Epoch 25\n",
    "    84% Validation Accuracy @Epoch 50\n",
    "    87% Validation Accuracy @Epoch 100\n",
    "\n",
    "    38ms/step on an RTX 2070\n",
    "    64 minutes and 20 sec to complete 100 Epochs\n",
    "    1000 steps per Epoch\n",
    "\n",
    "Without Augmentation:\n",
    "    75% Validation Accuracy @Epoch 15\n",
    "    78% Validation Accuracy @Epoch 25\n",
    "    81% Validation Accuracy @Epoch 50\n",
    "    83% Validation Accuracy @Epoch 100\n",
    "    \n",
    "    ~318us/step on an RTX 2070\n",
    "    26 minutes and 21 seconds to complete 100 Epochs\n",
    "    50000 steps per Epoch\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical, plot_model, Sequence\n",
    "from keras import backend as K\n",
    "import os\n",
    "\n",
    "if K.backend() == 'tensorflow': #prevents out CUDA out of memory\n",
    "    import tensorflow as tf\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "stime = time.time()\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "batch_size = 128 \n",
    "#WARNING: This depth setup (256->512->512) utilizes 90% of the GPU using an RTX 2070  \n",
    "depth1 = 256 \n",
    "depth2 = 512\n",
    "depth3 = 512\n",
    "hidden_units = 1024\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "num_predictions = 20\n",
    "image_size = x_train.shape[1]\n",
    "input_shape = (image_size, image_size, 3)\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(depth1, (3, 3), padding='same', activation = 'relu', input_shape=x_train.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(depth2, (3, 3), padding='same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(depth2, (3, 3), padding='same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "plot_model(model, to_file='cnn-mnist.png', show_shapes=True)\n",
    "\n",
    "\n",
    "opt = keras.optimizers.adam(lr=0.0001, decay=1e-6)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "data_augmentation = True\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    datagen = ImageDataGenerator(\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        width_shift_range=0.1, # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1, # randomly shift images vertically (fraction of total height)\n",
    "        fill_mode='nearest',\n",
    "        horizontal_flip=True  # randomly flip images\n",
    "        )\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "        batch_size=batch_size),\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        use_multiprocessing = True,\n",
    "        workers=12, #number of cores\n",
    "        steps_per_epoch = 1000)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "etime = time.time()\n",
    "print('Time taken in min: ', (etime-stime)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
