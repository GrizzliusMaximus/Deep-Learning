{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoE197z_Project1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrizzliusMaximus/Deep-Learning/blob/master/Project1/CoE197z_Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wLvO1AMY-iG3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#TRAINING DATA\n",
        "cleaning"
      ]
    },
    {
      "metadata": {
        "id": "Mq3GBCrFgTC4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "train = pd.read_csv('4910797b-ee55-40a7-8668-10efd5c1b960.csv')\n",
        "labels = pd.read_csv('0bf8bc6e-30d0-4c50-956a-603fc693d966.csv')\n",
        "#test = pd.read_csv('702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv')\n",
        "#data = pd.read_csv('702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv')\n",
        "data = labels.merge(train, on='id')\n",
        "\n",
        "\n",
        "\n",
        "data['gps_height'].replace(0.0, np.nan, inplace=True)\n",
        "data['population'].replace(0.0, np.nan, inplace=True)\n",
        "data['amount_tsh'].replace(0.0, np.nan, inplace=True)\n",
        "data['longitude'].replace(0.0, np.nan, inplace=True)\n",
        "data['latitude'].replace(0.0, np.nan, inplace=True)\n",
        "data['construction_year'].replace(0.0, np.nan, inplace=True)\n",
        "\n",
        "\n",
        "data.groupby(['region','permit']).size()\n",
        "data[\"gps_height\"].fillna(data.groupby(['region', 'district_code'])[\"gps_height\"].transform(\"mean\"), inplace=True)\n",
        "data[\"gps_height\"].fillna(data.groupby(['region'])[\"gps_height\"].transform(\"mean\"), inplace=True)\n",
        "data[\"gps_height\"].fillna(data[\"gps_height\"].mean(), inplace=True)\n",
        "data[\"population\"].fillna(data.groupby(['region', 'district_code'])[\"population\"].transform(\"median\"), inplace=True)\n",
        "data[\"population\"].fillna(data.groupby(['region'])[\"population\"].transform(\"median\"), inplace=True)\n",
        "data[\"population\"].fillna(data[\"population\"].median(), inplace=True)\n",
        "data[\"amount_tsh\"].fillna(data.groupby(['region', 'district_code'])[\"amount_tsh\"].transform(\"median\"), inplace=True)\n",
        "data[\"amount_tsh\"].fillna(data.groupby(['region'])[\"amount_tsh\"].transform(\"median\"), inplace=True)\n",
        "data[\"amount_tsh\"].fillna(data[\"amount_tsh\"].median(), inplace=True)\n",
        "data.groupby(['district_code', 'region','construction_year']).size()\n",
        "data[\"latitude\"].fillna(data.groupby(['region', 'district_code'])[\"latitude\"].transform(\"mean\"), inplace=True)\n",
        "data[\"longitude\"].fillna(data.groupby(['region', 'district_code'])[\"longitude\"].transform(\"mean\"), inplace=True)\n",
        "data[\"longitude\"].fillna(data.groupby(['region'])[\"longitude\"].transform(\"mean\"), inplace=True)\n",
        "data[\"construction_year\"].fillna(data.groupby(['region', 'district_code'])[\"construction_year\"].transform(\"median\"), inplace=True)\n",
        "data[\"construction_year\"].fillna(data.groupby(['region'])[\"construction_year\"].transform(\"median\"), inplace=True)\n",
        "data[\"construction_year\"].fillna(data.groupby(['district_code'])[\"construction_year\"].transform(\"median\"), inplace=True)\n",
        "data[\"construction_year\"].fillna(data[\"construction_year\"].median(), inplace=True)\n",
        "\n",
        "\n",
        "features=['amount_tsh', 'gps_height', 'population']\n",
        "scaler = MinMaxScaler(feature_range=(0,20))\n",
        "data[features] = scaler.fit_transform(data[features])\n",
        "\n",
        "\n",
        "data.isnull().sum()\n",
        "data['date_recorded'] = pd.to_datetime(data['date_recorded'])\n",
        "data['operational_year'] = data.date_recorded.dt.year - data.construction_year\n",
        "\n",
        "\n",
        "waste_features=['wpt_name','num_private','subvillage','region_code','recorded_by']\n",
        "#waste_features=['wpt_name','num_private','subvillage','region_code','recorded_by', 'latitude', 'longitude']\n",
        "data.drop(waste_features,axis=1,inplace=True)\n",
        "data.groupby(['management', 'management_group']).size()\n",
        "data.drop('management_group',axis=1,inplace=True)\n",
        "data.groupby(['extraction_type_group','extraction_type_class','extraction_type']).size()\n",
        "data.drop('extraction_type_group',axis=1,inplace=True)\n",
        "data.drop('extraction_type_class',axis=1,inplace=True)\n",
        "data.groupby(['scheme_management', 'scheme_name']).size() \n",
        "data.drop('scheme_name',axis=1,inplace=True)\n",
        "data.groupby(['payment', 'payment_type']).size()\n",
        "data.drop('payment',axis=1,inplace=True)\n",
        "data.groupby(['water_quality', 'quality_group']).size()\n",
        "data.drop('quality_group',axis=1,inplace=True)\n",
        "data.groupby(['quantity', 'quantity_group']).size() \n",
        "data.drop('quantity_group',axis=1,inplace=True)\n",
        "data.groupby(['source', 'source_type', 'source_class']).size()\n",
        "data.drop('source_type',axis=1,inplace=True)\n",
        "data.drop('source_class',axis=1,inplace=True)\n",
        "data.groupby(['waterpoint_type', 'waterpoint_type_group']).size() \n",
        "data.drop('waterpoint_type_group',axis=1,inplace=True)\n",
        "data.groupby(['lga', 'ward']).size()\n",
        "data.drop('ward',axis=1,inplace=True)\n",
        "data.groupby(['installer', 'funder']).size()\n",
        "data.drop('installer',axis=1,inplace=True)\n",
        "data.drop('public_meeting', axis=1, inplace=True)\n",
        "data.drop('permit', axis=1, inplace=True)\n",
        "\n",
        "data.waterpoint_type = data.waterpoint_type.str.lower()\n",
        "data.funder = data.funder.str.lower()\n",
        "data.basin = data.basin.str.lower()\n",
        "data.region = data.region.str.lower()\n",
        "data.source = data.source.str.lower()\n",
        "data.lga = data.lga.str.lower()\n",
        "data.management = data.management.str.lower()\n",
        "data.quantity = data.quantity.str.lower()\n",
        "data.water_quality = data.water_quality.str.lower()\n",
        "data.payment_type=data.payment_type.str.lower()\n",
        "data.extraction_type=data.extraction_type.str.lower()\n",
        "data[\"funder\"].fillna(\"other\", inplace=True)\n",
        "data[\"scheme_management\"].fillna(\"other\", inplace=True)\n",
        "#data[\"installer\"].fillna(\"other\", inplace=True)\n",
        "data.drop('date_recorded', axis=1, inplace=True)\n",
        "data.drop('construction_year', axis=1, inplace=True)\n",
        "pd.DataFrame(data).to_csv(\"clean.csv\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nMe1aSY9-zM9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Test Data\n",
        "Cleaning"
      ]
    },
    {
      "metadata": {
        "id": "CswCr2e-9msm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "data = pd.read_csv('702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv')\n",
        "#labels = pd.read_csv('0bf8bc6e-30d0-4c50-956a-603fc693d966.csv')\n",
        "#train = pd.read_csv('702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv')\n",
        "#data = pd.read_csv('702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv')\n",
        "#data = labels.merge(train, on='id')\n",
        "\n",
        "\n",
        "\n",
        "data['gps_height'].replace(0.0, np.nan, inplace=True)\n",
        "data['population'].replace(0.0, np.nan, inplace=True)\n",
        "data['amount_tsh'].replace(0.0, np.nan, inplace=True)\n",
        "data['longitude'].replace(0.0, np.nan, inplace=True)\n",
        "data['latitude'].replace(0.0, np.nan, inplace=True)\n",
        "data['construction_year'].replace(0.0, np.nan, inplace=True)\n",
        "\n",
        "\n",
        "data.groupby(['region','permit']).size()\n",
        "data[\"gps_height\"].fillna(data.groupby(['region', 'district_code'])[\"gps_height\"].transform(\"mean\"), inplace=True)\n",
        "data[\"gps_height\"].fillna(data.groupby(['region'])[\"gps_height\"].transform(\"mean\"), inplace=True)\n",
        "data[\"gps_height\"].fillna(data[\"gps_height\"].mean(), inplace=True)\n",
        "data[\"population\"].fillna(data.groupby(['region', 'district_code'])[\"population\"].transform(\"median\"), inplace=True)\n",
        "data[\"population\"].fillna(data.groupby(['region'])[\"population\"].transform(\"median\"), inplace=True)\n",
        "data[\"population\"].fillna(data[\"population\"].median(), inplace=True)\n",
        "data[\"amount_tsh\"].fillna(data.groupby(['region', 'district_code'])[\"amount_tsh\"].transform(\"median\"), inplace=True)\n",
        "data[\"amount_tsh\"].fillna(data.groupby(['region'])[\"amount_tsh\"].transform(\"median\"), inplace=True)\n",
        "data[\"amount_tsh\"].fillna(data[\"amount_tsh\"].median(), inplace=True)\n",
        "data.groupby(['district_code', 'region','construction_year']).size()\n",
        "data[\"latitude\"].fillna(data.groupby(['region', 'district_code'])[\"latitude\"].transform(\"mean\"), inplace=True)\n",
        "data[\"longitude\"].fillna(data.groupby(['region', 'district_code'])[\"longitude\"].transform(\"mean\"), inplace=True)\n",
        "data[\"longitude\"].fillna(data.groupby(['region'])[\"longitude\"].transform(\"mean\"), inplace=True)\n",
        "data[\"construction_year\"].fillna(data.groupby(['region', 'district_code'])[\"construction_year\"].transform(\"median\"), inplace=True)\n",
        "data[\"construction_year\"].fillna(data.groupby(['region'])[\"construction_year\"].transform(\"median\"), inplace=True)\n",
        "data[\"construction_year\"].fillna(data.groupby(['district_code'])[\"construction_year\"].transform(\"median\"), inplace=True)\n",
        "data[\"construction_year\"].fillna(data[\"construction_year\"].median(), inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "features=['amount_tsh', 'gps_height', 'population']\n",
        "scaler = MinMaxScaler(feature_range=(0,20))\n",
        "data[features] = scaler.fit_transform(data[features])\n",
        "\n",
        "\n",
        "data.isnull().sum()\n",
        "data['date_recorded'] = pd.to_datetime(data['date_recorded'])\n",
        "data['operational_year'] = data.date_recorded.dt.year - data.construction_year\n",
        "\n",
        "\n",
        "\n",
        "waste_features=['wpt_name','num_private','subvillage','region_code','recorded_by']\n",
        "data.drop(waste_features,axis=1,inplace=True)\n",
        "data.groupby(['management', 'management_group']).size()\n",
        "data.drop('management_group',axis=1,inplace=True)\n",
        "data.groupby(['extraction_type_group','extraction_type_class','extraction_type']).size()\n",
        "data.drop('extraction_type_group',axis=1,inplace=True)\n",
        "data.drop('extraction_type_class',axis=1,inplace=True)\n",
        "data.groupby(['scheme_management', 'scheme_name']).size() \n",
        "data.drop('scheme_name',axis=1,inplace=True)\n",
        "data.groupby(['payment', 'payment_type']).size()\n",
        "data.drop('payment',axis=1,inplace=True)\n",
        "data.groupby(['water_quality', 'quality_group']).size()\n",
        "data.drop('quality_group',axis=1,inplace=True)\n",
        "data.groupby(['quantity', 'quantity_group']).size() \n",
        "data.drop('quantity_group',axis=1,inplace=True)\n",
        "data.groupby(['source', 'source_type', 'source_class']).size()\n",
        "data.drop('source_type',axis=1,inplace=True)\n",
        "data.drop('source_class',axis=1,inplace=True)\n",
        "data.groupby(['waterpoint_type', 'waterpoint_type_group']).size() \n",
        "data.drop('waterpoint_type_group',axis=1,inplace=True)\n",
        "data.groupby(['lga', 'ward']).size()\n",
        "data.drop('ward',axis=1,inplace=True)\n",
        "data.groupby(['installer', 'funder']).size()\n",
        "data.drop('installer',axis=1,inplace=True)\n",
        "data.drop('public_meeting', axis=1, inplace=True)\n",
        "data.drop('permit', axis=1, inplace=True)\n",
        "\n",
        "data.waterpoint_type = data.waterpoint_type.str.lower()\n",
        "data.funder = data.funder.str.lower()\n",
        "data.basin = data.basin.str.lower()\n",
        "data.region = data.region.str.lower()\n",
        "data.source = data.source.str.lower()\n",
        "data.lga = data.lga.str.lower()\n",
        "data.management = data.management.str.lower()\n",
        "data.quantity = data.quantity.str.lower()\n",
        "data.water_quality = data.water_quality.str.lower()\n",
        "data.payment_type=data.payment_type.str.lower()\n",
        "data.extraction_type=data.extraction_type.str.lower()\n",
        "data[\"funder\"].fillna(\"other\", inplace=True)\n",
        "data[\"scheme_management\"].fillna(\"other\", inplace=True)\n",
        "#data[\"installer\"].fillna(\"other\", inplace=True)\n",
        "data.drop('date_recorded', axis=1, inplace=True)\n",
        "data.drop('construction_year', axis=1, inplace=True)\n",
        "pd.DataFrame(data).to_csv(\"clean_test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "01A94JwA-9Lh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#MODEL MLP with ADAM"
      ]
    },
    {
      "metadata": {
        "id": "GQcpYd9Cg_bC",
        "colab_type": "code",
        "outputId": "8e37d549-8e19-4ab7-ce5f-ab2db69184f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4566
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train = pd.read_csv('clean.csv')\n",
        "\n",
        "\n",
        "test = pd.read_csv('clean_test.csv')\n",
        "\n",
        "\n",
        "train.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "test.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "# target=train.pop('status_group')\n",
        "\n",
        "\n",
        "train['train']=1\n",
        "test['train']=0\n",
        "\n",
        "\n",
        "combined = pd.concat([train,test])\n",
        "\n",
        "combined['funder'] = pd.factorize(combined['funder'])[0]\n",
        "combined['scheme_management'] = pd.factorize(combined['scheme_management'])[0]\n",
        "combined['extraction_type'] = pd.factorize(combined['extraction_type'])[0]\n",
        "combined['management'] = pd.factorize(combined['management'])[0]\n",
        "combined['payment_type'] = pd.factorize(combined['payment_type'])[0]\n",
        "combined['water_quality'] = pd.factorize(combined['water_quality'])[0]\n",
        "combined['quantity'] = pd.factorize(combined['quantity'])[0]\n",
        "combined['source'] = pd.factorize(combined['source'])[0]\n",
        "combined['waterpoint_type'] = pd.factorize(combined['waterpoint_type'])[0]\n",
        "combined['basin'] = pd.factorize(combined['basin'])[0]\n",
        "combined['region'] = pd.factorize(combined['region'])[0]\n",
        "combined['lga'] = pd.factorize(combined['lga'])[0]\n",
        "combined['district_code'] = pd.factorize(combined['district_code'])[0]\n",
        "combined['operational_year'] = pd.factorize(combined['operational_year'])[0]\n",
        "combined['status_group'] = pd.factorize(combined['status_group'])[0]\n",
        "\n",
        "\n",
        "train_df = combined[combined[\"train\"] == 1]\n",
        "test_df = combined[combined[\"train\"] == 0]\n",
        "train_df.drop([\"train\"], axis=1, inplace=True)\n",
        "train_df.drop(['id'],axis=1, inplace=True)\n",
        "test_df.drop([\"train\"], axis=1, inplace=True)\n",
        "\n",
        "train = train_df.sample(frac=0.8)\n",
        "validate = train_df.sample(frac=0.1)\n",
        "test = train_df.sample(frac=0.1)\n",
        "\n",
        "[xsize, ysize] =train.shape\n",
        "\n",
        "train_target = train.pop('status_group')\n",
        "validate_target = validate.pop('status_group')\n",
        "test_target = test.pop('status_group')\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "num_labels = len(np.unique(train_target))\n",
        "train_target_cat = to_categorical(train_target)\n",
        "validate_target_cat = to_categorical(validate_target)\n",
        "test_target_cat = to_categorical(test_target)\n",
        "\n",
        "batch_size = 128\n",
        "hidden_units = 256\n",
        "dropout = 0.1\n",
        "input_size = (1 * ysize-1 * 1)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train, train_target_cat,  validation_data=(validate, validate_target_cat), epochs=100, batch_size=batch_size)\n",
        "score = model.evaluate(test, test_target_cat, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_61 (Dense)             (None, 256)               5120      \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_62 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 3)                 771       \n",
            "_________________________________________________________________\n",
            "activation_64 (Activation)   (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 137,475\n",
            "Trainable params: 137,475\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 47520 samples, validate on 5940 samples\n",
            "Epoch 1/100\n",
            "47520/47520 [==============================] - 5s 110us/step - loss: 1.1007 - acc: 0.5495 - val_loss: 0.8269 - val_acc: 0.6150\n",
            "Epoch 2/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.8287 - acc: 0.6086 - val_loss: 0.7795 - val_acc: 0.6365\n",
            "Epoch 3/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.7957 - acc: 0.6320 - val_loss: 0.7740 - val_acc: 0.6414\n",
            "Epoch 4/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.7782 - acc: 0.6410 - val_loss: 0.7428 - val_acc: 0.6628\n",
            "Epoch 5/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.7609 - acc: 0.6550 - val_loss: 0.7255 - val_acc: 0.6638\n",
            "Epoch 6/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.7482 - acc: 0.6620 - val_loss: 0.7321 - val_acc: 0.6662\n",
            "Epoch 7/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.7369 - acc: 0.6684 - val_loss: 0.7093 - val_acc: 0.6800\n",
            "Epoch 8/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.7223 - acc: 0.6766 - val_loss: 0.6917 - val_acc: 0.6907\n",
            "Epoch 9/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.7091 - acc: 0.6864 - val_loss: 0.6777 - val_acc: 0.6971\n",
            "Epoch 10/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.6987 - acc: 0.6931 - val_loss: 0.6523 - val_acc: 0.7128\n",
            "Epoch 11/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.6803 - acc: 0.7012 - val_loss: 0.6379 - val_acc: 0.7215\n",
            "Epoch 12/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.6714 - acc: 0.7064 - val_loss: 0.6264 - val_acc: 0.7259\n",
            "Epoch 13/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.6592 - acc: 0.7127 - val_loss: 0.6097 - val_acc: 0.7340\n",
            "Epoch 14/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.6496 - acc: 0.7180 - val_loss: 0.6094 - val_acc: 0.7330\n",
            "Epoch 15/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.6378 - acc: 0.7245 - val_loss: 0.5884 - val_acc: 0.7443\n",
            "Epoch 16/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.6290 - acc: 0.7299 - val_loss: 0.5980 - val_acc: 0.7399\n",
            "Epoch 17/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.6211 - acc: 0.7324 - val_loss: 0.5747 - val_acc: 0.7475\n",
            "Epoch 18/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.6154 - acc: 0.7348 - val_loss: 0.5685 - val_acc: 0.7502\n",
            "Epoch 19/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.6054 - acc: 0.7396 - val_loss: 0.5612 - val_acc: 0.7609\n",
            "Epoch 20/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.6017 - acc: 0.7424 - val_loss: 0.5743 - val_acc: 0.7460\n",
            "Epoch 21/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5977 - acc: 0.7457 - val_loss: 0.5449 - val_acc: 0.7652\n",
            "Epoch 22/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5906 - acc: 0.7469 - val_loss: 0.5434 - val_acc: 0.7692\n",
            "Epoch 23/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5862 - acc: 0.7483 - val_loss: 0.5351 - val_acc: 0.7732\n",
            "Epoch 24/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5822 - acc: 0.7521 - val_loss: 0.5383 - val_acc: 0.7726\n",
            "Epoch 25/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5766 - acc: 0.7535 - val_loss: 0.5363 - val_acc: 0.7729\n",
            "Epoch 26/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5768 - acc: 0.7528 - val_loss: 0.5341 - val_acc: 0.7648\n",
            "Epoch 27/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5707 - acc: 0.7557 - val_loss: 0.5316 - val_acc: 0.7658\n",
            "Epoch 28/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.5655 - acc: 0.7584 - val_loss: 0.5267 - val_acc: 0.7705\n",
            "Epoch 29/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5623 - acc: 0.7602 - val_loss: 0.5201 - val_acc: 0.7761\n",
            "Epoch 30/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5613 - acc: 0.7600 - val_loss: 0.5249 - val_acc: 0.7773\n",
            "Epoch 31/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.5541 - acc: 0.7635 - val_loss: 0.5133 - val_acc: 0.7833\n",
            "Epoch 32/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.5516 - acc: 0.7650 - val_loss: 0.5112 - val_acc: 0.7838\n",
            "Epoch 33/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5490 - acc: 0.7656 - val_loss: 0.5132 - val_acc: 0.7837\n",
            "Epoch 34/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5471 - acc: 0.7682 - val_loss: 0.5112 - val_acc: 0.7870\n",
            "Epoch 35/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.5433 - acc: 0.7690 - val_loss: 0.5046 - val_acc: 0.7875\n",
            "Epoch 36/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5441 - acc: 0.7680 - val_loss: 0.4965 - val_acc: 0.7916\n",
            "Epoch 37/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5387 - acc: 0.7715 - val_loss: 0.5071 - val_acc: 0.7877\n",
            "Epoch 38/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.5374 - acc: 0.7695 - val_loss: 0.5059 - val_acc: 0.7889\n",
            "Epoch 39/100\n",
            "47520/47520 [==============================] - 3s 69us/step - loss: 0.5339 - acc: 0.7719 - val_loss: 0.4935 - val_acc: 0.7951\n",
            "Epoch 40/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5293 - acc: 0.7751 - val_loss: 0.4883 - val_acc: 0.7921\n",
            "Epoch 41/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5323 - acc: 0.7743 - val_loss: 0.4877 - val_acc: 0.7955\n",
            "Epoch 42/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.5258 - acc: 0.7757 - val_loss: 0.4847 - val_acc: 0.7966\n",
            "Epoch 43/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.5271 - acc: 0.7739 - val_loss: 0.4901 - val_acc: 0.7928\n",
            "Epoch 44/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5240 - acc: 0.7776 - val_loss: 0.4908 - val_acc: 0.7896\n",
            "Epoch 45/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5196 - acc: 0.7770 - val_loss: 0.4922 - val_acc: 0.7936\n",
            "Epoch 46/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5202 - acc: 0.7777 - val_loss: 0.4814 - val_acc: 0.7956\n",
            "Epoch 47/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5202 - acc: 0.7774 - val_loss: 0.4771 - val_acc: 0.8010\n",
            "Epoch 48/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.5153 - acc: 0.7810 - val_loss: 0.4838 - val_acc: 0.7983\n",
            "Epoch 49/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.5136 - acc: 0.7812 - val_loss: 0.4745 - val_acc: 0.8010\n",
            "Epoch 50/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5150 - acc: 0.7822 - val_loss: 0.4783 - val_acc: 0.7983\n",
            "Epoch 51/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5132 - acc: 0.7805 - val_loss: 0.4767 - val_acc: 0.8015\n",
            "Epoch 52/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5096 - acc: 0.7814 - val_loss: 0.4709 - val_acc: 0.8027\n",
            "Epoch 53/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5100 - acc: 0.7833 - val_loss: 0.4641 - val_acc: 0.8084\n",
            "Epoch 54/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.5086 - acc: 0.7832 - val_loss: 0.4780 - val_acc: 0.8029\n",
            "Epoch 55/100\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.5050 - acc: 0.7851 - val_loss: 0.4694 - val_acc: 0.8045\n",
            "Epoch 56/100\n",
            "47520/47520 [==============================] - 4s 78us/step - loss: 0.5043 - acc: 0.7860 - val_loss: 0.4678 - val_acc: 0.8082\n",
            "Epoch 57/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5028 - acc: 0.7842 - val_loss: 0.4713 - val_acc: 0.8015\n",
            "Epoch 58/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.5018 - acc: 0.7850 - val_loss: 0.4632 - val_acc: 0.8061\n",
            "Epoch 59/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.5009 - acc: 0.7875 - val_loss: 0.4660 - val_acc: 0.8054\n",
            "Epoch 60/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4987 - acc: 0.7899 - val_loss: 0.4574 - val_acc: 0.8093\n",
            "Epoch 61/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4961 - acc: 0.7880 - val_loss: 0.4649 - val_acc: 0.8069\n",
            "Epoch 62/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.4934 - acc: 0.7890 - val_loss: 0.4608 - val_acc: 0.8086\n",
            "Epoch 63/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.4952 - acc: 0.7894 - val_loss: 0.4592 - val_acc: 0.8040\n",
            "Epoch 64/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.4928 - acc: 0.7911 - val_loss: 0.4532 - val_acc: 0.8146\n",
            "Epoch 65/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.4909 - acc: 0.7904 - val_loss: 0.4555 - val_acc: 0.8094\n",
            "Epoch 66/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.4913 - acc: 0.7925 - val_loss: 0.4580 - val_acc: 0.8116\n",
            "Epoch 67/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.4927 - acc: 0.7912 - val_loss: 0.4648 - val_acc: 0.8093\n",
            "Epoch 68/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.4898 - acc: 0.7915 - val_loss: 0.4600 - val_acc: 0.8082\n",
            "Epoch 69/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.4883 - acc: 0.7913 - val_loss: 0.4523 - val_acc: 0.8162\n",
            "Epoch 70/100\n",
            "47520/47520 [==============================] - 3s 69us/step - loss: 0.4861 - acc: 0.7933 - val_loss: 0.4527 - val_acc: 0.8116\n",
            "Epoch 71/100\n",
            "47520/47520 [==============================] - 3s 69us/step - loss: 0.4902 - acc: 0.7908 - val_loss: 0.4518 - val_acc: 0.8165\n",
            "Epoch 72/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.4867 - acc: 0.7928 - val_loss: 0.4450 - val_acc: 0.8162\n",
            "Epoch 73/100\n",
            "47520/47520 [==============================] - 3s 69us/step - loss: 0.4831 - acc: 0.7939 - val_loss: 0.4536 - val_acc: 0.8140\n",
            "Epoch 74/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.4825 - acc: 0.7943 - val_loss: 0.4545 - val_acc: 0.8125\n",
            "Epoch 75/100\n",
            "47520/47520 [==============================] - 3s 69us/step - loss: 0.4816 - acc: 0.7952 - val_loss: 0.4519 - val_acc: 0.8141\n",
            "Epoch 76/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.4830 - acc: 0.7929 - val_loss: 0.4467 - val_acc: 0.8152\n",
            "Epoch 77/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.4813 - acc: 0.7960 - val_loss: 0.4528 - val_acc: 0.8133\n",
            "Epoch 78/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4818 - acc: 0.7933 - val_loss: 0.4458 - val_acc: 0.8197\n",
            "Epoch 79/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4798 - acc: 0.7952 - val_loss: 0.4439 - val_acc: 0.8173\n",
            "Epoch 80/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.4805 - acc: 0.7958 - val_loss: 0.4471 - val_acc: 0.8170\n",
            "Epoch 81/100\n",
            "47520/47520 [==============================] - 4s 79us/step - loss: 0.4769 - acc: 0.7980 - val_loss: 0.4473 - val_acc: 0.8138\n",
            "Epoch 82/100\n",
            "47520/47520 [==============================] - 4s 78us/step - loss: 0.4782 - acc: 0.7963 - val_loss: 0.4497 - val_acc: 0.8140\n",
            "Epoch 83/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.4769 - acc: 0.7952 - val_loss: 0.4417 - val_acc: 0.8172\n",
            "Epoch 84/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.4751 - acc: 0.7976 - val_loss: 0.4432 - val_acc: 0.8253\n",
            "Epoch 85/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4743 - acc: 0.7981 - val_loss: 0.4431 - val_acc: 0.8163\n",
            "Epoch 86/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.4739 - acc: 0.7967 - val_loss: 0.4388 - val_acc: 0.8207\n",
            "Epoch 87/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.4743 - acc: 0.7976 - val_loss: 0.4472 - val_acc: 0.8145\n",
            "Epoch 88/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4725 - acc: 0.7999 - val_loss: 0.4450 - val_acc: 0.8237\n",
            "Epoch 89/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.4713 - acc: 0.7993 - val_loss: 0.4421 - val_acc: 0.8190\n",
            "Epoch 90/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.4721 - acc: 0.8004 - val_loss: 0.4433 - val_acc: 0.8178\n",
            "Epoch 91/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.4714 - acc: 0.7991 - val_loss: 0.4445 - val_acc: 0.8175\n",
            "Epoch 92/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.4695 - acc: 0.8003 - val_loss: 0.4392 - val_acc: 0.8231\n",
            "Epoch 93/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4680 - acc: 0.8010 - val_loss: 0.4397 - val_acc: 0.8172\n",
            "Epoch 94/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4672 - acc: 0.8000 - val_loss: 0.4424 - val_acc: 0.8141\n",
            "Epoch 95/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4702 - acc: 0.7997 - val_loss: 0.4395 - val_acc: 0.8237\n",
            "Epoch 96/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4670 - acc: 0.7994 - val_loss: 0.4402 - val_acc: 0.8177\n",
            "Epoch 97/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.4675 - acc: 0.8006 - val_loss: 0.4308 - val_acc: 0.8246\n",
            "Epoch 98/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4662 - acc: 0.8016 - val_loss: 0.4322 - val_acc: 0.8256\n",
            "Epoch 99/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.4626 - acc: 0.8033 - val_loss: 0.4322 - val_acc: 0.8234\n",
            "Epoch 100/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4652 - acc: 0.8022 - val_loss: 0.4343 - val_acc: 0.8224\n",
            "5940/5940 [==============================] - 0s 19us/step\n",
            "\n",
            "Test accuracy: 81.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YzNFPYic_97o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#MLP with SGD"
      ]
    },
    {
      "metadata": {
        "id": "Q-CL4Hl_ABRa",
        "colab_type": "code",
        "outputId": "02c50b36-5236-4da9-c5b0-297c2c0fa6e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4566
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train = pd.read_csv('clean.csv')\n",
        "\n",
        "\n",
        "test = pd.read_csv('clean_test.csv')\n",
        "\n",
        "\n",
        "train.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "test.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "# target=train.pop('status_group')\n",
        "\n",
        "\n",
        "train['train']=1\n",
        "test['train']=0\n",
        "\n",
        "\n",
        "combined = pd.concat([train,test])\n",
        "\n",
        "combined['funder'] = pd.factorize(combined['funder'])[0]\n",
        "combined['scheme_management'] = pd.factorize(combined['scheme_management'])[0]\n",
        "combined['extraction_type'] = pd.factorize(combined['extraction_type'])[0]\n",
        "combined['management'] = pd.factorize(combined['management'])[0]\n",
        "combined['payment_type'] = pd.factorize(combined['payment_type'])[0]\n",
        "combined['water_quality'] = pd.factorize(combined['water_quality'])[0]\n",
        "combined['quantity'] = pd.factorize(combined['quantity'])[0]\n",
        "combined['source'] = pd.factorize(combined['source'])[0]\n",
        "combined['waterpoint_type'] = pd.factorize(combined['waterpoint_type'])[0]\n",
        "combined['basin'] = pd.factorize(combined['basin'])[0]\n",
        "combined['region'] = pd.factorize(combined['region'])[0]\n",
        "combined['lga'] = pd.factorize(combined['lga'])[0]\n",
        "combined['district_code'] = pd.factorize(combined['district_code'])[0]\n",
        "combined['operational_year'] = pd.factorize(combined['operational_year'])[0]\n",
        "combined['status_group'] = pd.factorize(combined['status_group'])[0]\n",
        "\n",
        "\n",
        "train_df = combined[combined[\"train\"] == 1]\n",
        "test_df = combined[combined[\"train\"] == 0]\n",
        "train_df.drop([\"train\"], axis=1, inplace=True)\n",
        "train_df.drop(['id'],axis=1, inplace=True)\n",
        "test_df.drop([\"train\"], axis=1, inplace=True)\n",
        "\n",
        "train = train_df.sample(frac=0.8)\n",
        "validate = train_df.sample(frac=0.1)\n",
        "test = train_df.sample(frac=0.1)\n",
        "\n",
        "[xsize, ysize] =train.shape\n",
        "\n",
        "train_target = train.pop('status_group')\n",
        "validate_target = validate.pop('status_group')\n",
        "test_target = test.pop('status_group')\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "num_labels = len(np.unique(train_target))\n",
        "train_target_cat = to_categorical(train_target)\n",
        "validate_target_cat = to_categorical(validate_target)\n",
        "test_target_cat = to_categorical(test_target)\n",
        "\n",
        "batch_size = 128\n",
        "hidden_units = 256\n",
        "dropout = 0.1\n",
        "input_size = (1 * ysize-1 * 1)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train, train_target_cat,  validation_data=(validate, validate_target_cat), epochs=100, batch_size=batch_size)\n",
        "score = model.evaluate(test, test_target_cat, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_57 (Dense)             (None, 256)               5120      \n",
            "_________________________________________________________________\n",
            "activation_57 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_58 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_59 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 3)                 771       \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 137,475\n",
            "Trainable params: 137,475\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 47520 samples, validate on 5940 samples\n",
            "Epoch 1/100\n",
            "47520/47520 [==============================] - 5s 98us/step - loss: 1.0674 - acc: 0.5335 - val_loss: 0.8707 - val_acc: 0.5640\n",
            "Epoch 2/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.8734 - acc: 0.5636 - val_loss: 0.8676 - val_acc: 0.5608\n",
            "Epoch 3/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.8603 - acc: 0.5754 - val_loss: 0.8429 - val_acc: 0.5995\n",
            "Epoch 4/100\n",
            "47520/47520 [==============================] - 3s 65us/step - loss: 0.8482 - acc: 0.5858 - val_loss: 0.8302 - val_acc: 0.6049\n",
            "Epoch 5/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.8430 - acc: 0.5911 - val_loss: 0.8415 - val_acc: 0.5860\n",
            "Epoch 6/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.8354 - acc: 0.5959 - val_loss: 0.8185 - val_acc: 0.6093\n",
            "Epoch 7/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.8300 - acc: 0.6000 - val_loss: 0.8297 - val_acc: 0.5943\n",
            "Epoch 8/100\n",
            "47520/47520 [==============================] - 3s 69us/step - loss: 0.8270 - acc: 0.6045 - val_loss: 0.8458 - val_acc: 0.5658\n",
            "Epoch 9/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.8211 - acc: 0.6070 - val_loss: 0.8046 - val_acc: 0.6226\n",
            "Epoch 10/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.8173 - acc: 0.6090 - val_loss: 0.8227 - val_acc: 0.5973\n",
            "Epoch 11/100\n",
            "47520/47520 [==============================] - 3s 65us/step - loss: 0.8118 - acc: 0.6149 - val_loss: 0.8169 - val_acc: 0.6027\n",
            "Epoch 12/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.8109 - acc: 0.6144 - val_loss: 0.8009 - val_acc: 0.6258\n",
            "Epoch 13/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.8058 - acc: 0.6186 - val_loss: 0.7845 - val_acc: 0.6355\n",
            "Epoch 14/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.8044 - acc: 0.6203 - val_loss: 0.7887 - val_acc: 0.6271\n",
            "Epoch 15/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.8026 - acc: 0.6226 - val_loss: 0.7824 - val_acc: 0.6360\n",
            "Epoch 16/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7985 - acc: 0.6240 - val_loss: 0.7836 - val_acc: 0.6335\n",
            "Epoch 17/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7972 - acc: 0.6271 - val_loss: 0.7952 - val_acc: 0.6231\n",
            "Epoch 18/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7923 - acc: 0.6306 - val_loss: 0.7989 - val_acc: 0.6044\n",
            "Epoch 19/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7936 - acc: 0.6278 - val_loss: 0.7872 - val_acc: 0.6347\n",
            "Epoch 20/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7882 - acc: 0.6320 - val_loss: 0.7711 - val_acc: 0.6406\n",
            "Epoch 21/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7880 - acc: 0.6315 - val_loss: 0.7810 - val_acc: 0.6407\n",
            "Epoch 22/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7851 - acc: 0.6328 - val_loss: 0.7891 - val_acc: 0.6187\n",
            "Epoch 23/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7839 - acc: 0.6336 - val_loss: 0.7657 - val_acc: 0.6380\n",
            "Epoch 24/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7841 - acc: 0.6343 - val_loss: 0.7730 - val_acc: 0.6508\n",
            "Epoch 25/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7802 - acc: 0.6351 - val_loss: 0.7693 - val_acc: 0.6343\n",
            "Epoch 26/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7785 - acc: 0.6389 - val_loss: 0.7731 - val_acc: 0.6404\n",
            "Epoch 27/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7768 - acc: 0.6404 - val_loss: 0.7716 - val_acc: 0.6436\n",
            "Epoch 28/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7756 - acc: 0.6393 - val_loss: 0.7585 - val_acc: 0.6471\n",
            "Epoch 29/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7737 - acc: 0.6413 - val_loss: 0.7589 - val_acc: 0.6576\n",
            "Epoch 30/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7717 - acc: 0.6410 - val_loss: 0.7619 - val_acc: 0.6492\n",
            "Epoch 31/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7702 - acc: 0.6442 - val_loss: 0.8641 - val_acc: 0.5990\n",
            "Epoch 32/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7694 - acc: 0.6451 - val_loss: 0.7577 - val_acc: 0.6475\n",
            "Epoch 33/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7667 - acc: 0.6481 - val_loss: 0.8401 - val_acc: 0.5739\n",
            "Epoch 34/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7652 - acc: 0.6475 - val_loss: 0.7589 - val_acc: 0.6478\n",
            "Epoch 35/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7642 - acc: 0.6475 - val_loss: 0.7531 - val_acc: 0.6609\n",
            "Epoch 36/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7640 - acc: 0.6478 - val_loss: 0.7881 - val_acc: 0.6160\n",
            "Epoch 37/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7629 - acc: 0.6474 - val_loss: 0.7499 - val_acc: 0.6630\n",
            "Epoch 38/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7616 - acc: 0.6508 - val_loss: 0.7429 - val_acc: 0.6593\n",
            "Epoch 39/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7603 - acc: 0.6509 - val_loss: 0.7436 - val_acc: 0.6680\n",
            "Epoch 40/100\n",
            "47520/47520 [==============================] - 3s 61us/step - loss: 0.7584 - acc: 0.6499 - val_loss: 0.7503 - val_acc: 0.6480\n",
            "Epoch 41/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7575 - acc: 0.6520 - val_loss: 0.7373 - val_acc: 0.6668\n",
            "Epoch 42/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7544 - acc: 0.6544 - val_loss: 0.7625 - val_acc: 0.6502\n",
            "Epoch 43/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7555 - acc: 0.6562 - val_loss: 0.7540 - val_acc: 0.6562\n",
            "Epoch 44/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7544 - acc: 0.6528 - val_loss: 0.7415 - val_acc: 0.6604\n",
            "Epoch 45/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7507 - acc: 0.6559 - val_loss: 0.7346 - val_acc: 0.6650\n",
            "Epoch 46/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7512 - acc: 0.6593 - val_loss: 0.7415 - val_acc: 0.6668\n",
            "Epoch 47/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7493 - acc: 0.6594 - val_loss: 0.7449 - val_acc: 0.6596\n",
            "Epoch 48/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7472 - acc: 0.6603 - val_loss: 0.7561 - val_acc: 0.6598\n",
            "Epoch 49/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7471 - acc: 0.6578 - val_loss: 0.7761 - val_acc: 0.6348\n",
            "Epoch 50/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7468 - acc: 0.6604 - val_loss: 0.7386 - val_acc: 0.6675\n",
            "Epoch 51/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7427 - acc: 0.6610 - val_loss: 0.7333 - val_acc: 0.6633\n",
            "Epoch 52/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7438 - acc: 0.6614 - val_loss: 0.7484 - val_acc: 0.6537\n",
            "Epoch 53/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7431 - acc: 0.6624 - val_loss: 0.7252 - val_acc: 0.6741\n",
            "Epoch 54/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7418 - acc: 0.6646 - val_loss: 0.7589 - val_acc: 0.6564\n",
            "Epoch 55/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7397 - acc: 0.6638 - val_loss: 0.7387 - val_acc: 0.6660\n",
            "Epoch 56/100\n",
            "47520/47520 [==============================] - 3s 65us/step - loss: 0.7412 - acc: 0.6643 - val_loss: 0.7321 - val_acc: 0.6582\n",
            "Epoch 57/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7390 - acc: 0.6620 - val_loss: 0.7324 - val_acc: 0.6697\n",
            "Epoch 58/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7398 - acc: 0.6639 - val_loss: 0.7227 - val_acc: 0.6749\n",
            "Epoch 59/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7368 - acc: 0.6660 - val_loss: 0.7221 - val_acc: 0.6845\n",
            "Epoch 60/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7350 - acc: 0.6659 - val_loss: 0.7359 - val_acc: 0.6751\n",
            "Epoch 61/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7330 - acc: 0.6684 - val_loss: 0.7212 - val_acc: 0.6714\n",
            "Epoch 62/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7330 - acc: 0.6678 - val_loss: 0.7554 - val_acc: 0.6551\n",
            "Epoch 63/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7329 - acc: 0.6694 - val_loss: 0.7438 - val_acc: 0.6529\n",
            "Epoch 64/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7334 - acc: 0.6657 - val_loss: 0.7448 - val_acc: 0.6581\n",
            "Epoch 65/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7333 - acc: 0.6652 - val_loss: 0.7365 - val_acc: 0.6586\n",
            "Epoch 66/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7299 - acc: 0.6698 - val_loss: 0.7765 - val_acc: 0.6380\n",
            "Epoch 67/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7295 - acc: 0.6723 - val_loss: 0.7197 - val_acc: 0.6742\n",
            "Epoch 68/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7280 - acc: 0.6707 - val_loss: 0.7117 - val_acc: 0.6801\n",
            "Epoch 69/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7291 - acc: 0.6692 - val_loss: 0.7115 - val_acc: 0.6736\n",
            "Epoch 70/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7284 - acc: 0.6709 - val_loss: 0.7160 - val_acc: 0.6800\n",
            "Epoch 71/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7259 - acc: 0.6715 - val_loss: 0.7030 - val_acc: 0.6828\n",
            "Epoch 72/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7262 - acc: 0.6719 - val_loss: 0.7109 - val_acc: 0.6822\n",
            "Epoch 73/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7237 - acc: 0.6729 - val_loss: 0.7209 - val_acc: 0.6795\n",
            "Epoch 74/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7243 - acc: 0.6752 - val_loss: 0.7114 - val_acc: 0.6791\n",
            "Epoch 75/100\n",
            "47520/47520 [==============================] - 3s 61us/step - loss: 0.7236 - acc: 0.6732 - val_loss: 0.7042 - val_acc: 0.6798\n",
            "Epoch 76/100\n",
            "47520/47520 [==============================] - 3s 61us/step - loss: 0.7222 - acc: 0.6723 - val_loss: 0.7816 - val_acc: 0.6111\n",
            "Epoch 77/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7209 - acc: 0.6753 - val_loss: 0.7116 - val_acc: 0.6781\n",
            "Epoch 78/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7195 - acc: 0.6762 - val_loss: 0.7333 - val_acc: 0.6604\n",
            "Epoch 79/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7192 - acc: 0.6768 - val_loss: 0.7239 - val_acc: 0.6699\n",
            "Epoch 80/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7175 - acc: 0.6772 - val_loss: 0.7230 - val_acc: 0.6606\n",
            "Epoch 81/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7173 - acc: 0.6769 - val_loss: 0.7093 - val_acc: 0.6785\n",
            "Epoch 82/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7158 - acc: 0.6796 - val_loss: 0.7252 - val_acc: 0.6675\n",
            "Epoch 83/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7140 - acc: 0.6810 - val_loss: 0.6936 - val_acc: 0.6929\n",
            "Epoch 84/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7141 - acc: 0.6790 - val_loss: 0.7041 - val_acc: 0.6796\n",
            "Epoch 85/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.7126 - acc: 0.6782 - val_loss: 0.6921 - val_acc: 0.6916\n",
            "Epoch 86/100\n",
            "47520/47520 [==============================] - 3s 66us/step - loss: 0.7120 - acc: 0.6819 - val_loss: 0.6924 - val_acc: 0.6955\n",
            "Epoch 87/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7115 - acc: 0.6821 - val_loss: 0.6924 - val_acc: 0.6939\n",
            "Epoch 88/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7099 - acc: 0.6800 - val_loss: 0.7020 - val_acc: 0.6859\n",
            "Epoch 89/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7112 - acc: 0.6816 - val_loss: 0.7153 - val_acc: 0.6754\n",
            "Epoch 90/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7082 - acc: 0.6815 - val_loss: 0.6994 - val_acc: 0.6840\n",
            "Epoch 91/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7086 - acc: 0.6828 - val_loss: 0.7354 - val_acc: 0.6582\n",
            "Epoch 92/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7071 - acc: 0.6815 - val_loss: 0.6783 - val_acc: 0.6988\n",
            "Epoch 93/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7059 - acc: 0.6852 - val_loss: 0.6888 - val_acc: 0.6949\n",
            "Epoch 94/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7054 - acc: 0.6824 - val_loss: 0.6938 - val_acc: 0.6912\n",
            "Epoch 95/100\n",
            "47520/47520 [==============================] - 3s 61us/step - loss: 0.7050 - acc: 0.6852 - val_loss: 0.6853 - val_acc: 0.6955\n",
            "Epoch 96/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7050 - acc: 0.6842 - val_loss: 0.6763 - val_acc: 0.6981\n",
            "Epoch 97/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7048 - acc: 0.6844 - val_loss: 0.7074 - val_acc: 0.6658\n",
            "Epoch 98/100\n",
            "47520/47520 [==============================] - 3s 64us/step - loss: 0.7017 - acc: 0.6888 - val_loss: 0.6873 - val_acc: 0.7020\n",
            "Epoch 99/100\n",
            "47520/47520 [==============================] - 3s 63us/step - loss: 0.7019 - acc: 0.6854 - val_loss: 0.7015 - val_acc: 0.6882\n",
            "Epoch 100/100\n",
            "47520/47520 [==============================] - 3s 62us/step - loss: 0.7019 - acc: 0.6866 - val_loss: 0.6869 - val_acc: 0.6992\n",
            "5940/5940 [==============================] - 0s 18us/step\n",
            "\n",
            "Test accuracy: 69.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MS7bI0BVGz4N",
        "colab_type": "code",
        "outputId": "d3aaf761-f0e9-4bfb-c7ca-57b6a131c3e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4566
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras import optimizers\n",
        "import keras.backend as K\n",
        "\n",
        "train = pd.read_csv('clean.csv')\n",
        "\n",
        "\n",
        "test = pd.read_csv('clean_test.csv')\n",
        "\n",
        "\n",
        "train.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "test.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "# target=train.pop('status_group')\n",
        "\n",
        "\n",
        "train['train']=1\n",
        "test['train']=0\n",
        "\n",
        "\n",
        "combined = pd.concat([train,test])\n",
        "\n",
        "combined['funder'] = pd.factorize(combined['funder'])[0]\n",
        "combined['scheme_management'] = pd.factorize(combined['scheme_management'])[0]\n",
        "combined['extraction_type'] = pd.factorize(combined['extraction_type'])[0]\n",
        "combined['management'] = pd.factorize(combined['management'])[0]\n",
        "combined['payment_type'] = pd.factorize(combined['payment_type'])[0]\n",
        "combined['water_quality'] = pd.factorize(combined['water_quality'])[0]\n",
        "combined['quantity'] = pd.factorize(combined['quantity'])[0]\n",
        "combined['source'] = pd.factorize(combined['source'])[0]\n",
        "combined['waterpoint_type'] = pd.factorize(combined['waterpoint_type'])[0]\n",
        "combined['basin'] = pd.factorize(combined['basin'])[0]\n",
        "combined['region'] = pd.factorize(combined['region'])[0]\n",
        "combined['lga'] = pd.factorize(combined['lga'])[0]\n",
        "combined['district_code'] = pd.factorize(combined['district_code'])[0]\n",
        "combined['operational_year'] = pd.factorize(combined['operational_year'])[0]\n",
        "combined['status_group'] = pd.factorize(combined['status_group'])[0]\n",
        "\n",
        "\n",
        "train_df = combined[combined[\"train\"] == 1]\n",
        "test_df = combined[combined[\"train\"] == 0]\n",
        "train_df.drop([\"train\"], axis=1, inplace=True)\n",
        "train_df.drop(['id'],axis=1, inplace=True)\n",
        "test_df.drop([\"train\"], axis=1, inplace=True)\n",
        "\n",
        "train = train_df.sample(frac=0.8)\n",
        "validate = train_df.sample(frac=0.1)\n",
        "test = train_df.sample(frac=0.1)\n",
        "\n",
        "[xsize, ysize] =train.shape\n",
        "\n",
        "train_target = train.pop('status_group')\n",
        "validate_target = validate.pop('status_group')\n",
        "test_target = test.pop('status_group')\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "num_labels = len(np.unique(train_target))\n",
        "train_target_cat = to_categorical(train_target)\n",
        "validate_target_cat = to_categorical(validate_target)\n",
        "test_target_cat = to_categorical(test_target)\n",
        "\n",
        "batch_size = 128\n",
        "hidden_units = 256\n",
        "dropout = 0.1\n",
        "input_size = (1 * ysize-1 * 1)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.000, amsgrad=False)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train, train_target_cat,  validation_data=(validate, validate_target_cat), epochs=100, batch_size=batch_size)\n",
        "score = model.evaluate(test, test_target_cat, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_29 (Dense)             (None, 256)               5120      \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 3)                 771       \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 137,475\n",
            "Trainable params: 137,475\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 47520 samples, validate on 5940 samples\n",
            "Epoch 1/100\n",
            "47520/47520 [==============================] - 5s 97us/step - loss: 1.1111 - acc: 0.5496 - val_loss: 0.8437 - val_acc: 0.6099\n",
            "Epoch 2/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.8338 - acc: 0.6035 - val_loss: 0.8010 - val_acc: 0.6273\n",
            "Epoch 3/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.8001 - acc: 0.6254 - val_loss: 0.7771 - val_acc: 0.6441\n",
            "Epoch 4/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.7834 - acc: 0.6392 - val_loss: 0.7609 - val_acc: 0.6588\n",
            "Epoch 5/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.7659 - acc: 0.6496 - val_loss: 0.7426 - val_acc: 0.6684\n",
            "Epoch 6/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.7538 - acc: 0.6585 - val_loss: 0.7312 - val_acc: 0.6742\n",
            "Epoch 7/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.7406 - acc: 0.6652 - val_loss: 0.7085 - val_acc: 0.6918\n",
            "Epoch 8/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.7259 - acc: 0.6745 - val_loss: 0.7019 - val_acc: 0.6912\n",
            "Epoch 9/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.7116 - acc: 0.6842 - val_loss: 0.6804 - val_acc: 0.7071\n",
            "Epoch 10/100\n",
            "47520/47520 [==============================] - 4s 82us/step - loss: 0.6979 - acc: 0.6943 - val_loss: 0.6632 - val_acc: 0.7141\n",
            "Epoch 11/100\n",
            "47520/47520 [==============================] - 4s 78us/step - loss: 0.6845 - acc: 0.7004 - val_loss: 0.6513 - val_acc: 0.7197\n",
            "Epoch 12/100\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.6772 - acc: 0.7029 - val_loss: 0.6473 - val_acc: 0.7217\n",
            "Epoch 13/100\n",
            "47520/47520 [==============================] - 4s 78us/step - loss: 0.6664 - acc: 0.7105 - val_loss: 0.6316 - val_acc: 0.7263\n",
            "Epoch 14/100\n",
            "47520/47520 [==============================] - 4s 78us/step - loss: 0.6572 - acc: 0.7131 - val_loss: 0.6259 - val_acc: 0.7271\n",
            "Epoch 15/100\n",
            "47520/47520 [==============================] - 4s 78us/step - loss: 0.6449 - acc: 0.7189 - val_loss: 0.6070 - val_acc: 0.7394\n",
            "Epoch 16/100\n",
            "47520/47520 [==============================] - 4s 79us/step - loss: 0.6354 - acc: 0.7272 - val_loss: 0.5945 - val_acc: 0.7448\n",
            "Epoch 17/100\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.6271 - acc: 0.7276 - val_loss: 0.6086 - val_acc: 0.7426\n",
            "Epoch 18/100\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.6209 - acc: 0.7322 - val_loss: 0.5836 - val_acc: 0.7517\n",
            "Epoch 19/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.6156 - acc: 0.7341 - val_loss: 0.5791 - val_acc: 0.7495\n",
            "Epoch 20/100\n",
            "47520/47520 [==============================] - 4s 87us/step - loss: 0.6075 - acc: 0.7368 - val_loss: 0.5762 - val_acc: 0.7515\n",
            "Epoch 21/100\n",
            "47520/47520 [==============================] - 4s 81us/step - loss: 0.6019 - acc: 0.7431 - val_loss: 0.5674 - val_acc: 0.7535\n",
            "Epoch 22/100\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.5972 - acc: 0.7436 - val_loss: 0.5553 - val_acc: 0.7653\n",
            "Epoch 23/100\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.5905 - acc: 0.7466 - val_loss: 0.5528 - val_acc: 0.7626\n",
            "Epoch 24/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.5862 - acc: 0.7498 - val_loss: 0.5490 - val_acc: 0.7684\n",
            "Epoch 25/100\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.5836 - acc: 0.7501 - val_loss: 0.5464 - val_acc: 0.7684\n",
            "Epoch 26/100\n",
            "47520/47520 [==============================] - 4s 78us/step - loss: 0.5809 - acc: 0.7510 - val_loss: 0.5452 - val_acc: 0.7673\n",
            "Epoch 27/100\n",
            "47520/47520 [==============================] - 4s 78us/step - loss: 0.5758 - acc: 0.7536 - val_loss: 0.5420 - val_acc: 0.7684\n",
            "Epoch 28/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.5716 - acc: 0.7543 - val_loss: 0.5468 - val_acc: 0.7675\n",
            "Epoch 29/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.5672 - acc: 0.7560 - val_loss: 0.5352 - val_acc: 0.7717\n",
            "Epoch 30/100\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.5659 - acc: 0.7576 - val_loss: 0.5234 - val_acc: 0.7786\n",
            "Epoch 31/100\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.5609 - acc: 0.7582 - val_loss: 0.5263 - val_acc: 0.7779\n",
            "Epoch 32/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.5560 - acc: 0.7617 - val_loss: 0.5230 - val_acc: 0.7810\n",
            "Epoch 33/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.5526 - acc: 0.7618 - val_loss: 0.5176 - val_acc: 0.7795\n",
            "Epoch 34/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.5518 - acc: 0.7630 - val_loss: 0.5190 - val_acc: 0.7823\n",
            "Epoch 35/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.5481 - acc: 0.7652 - val_loss: 0.5200 - val_acc: 0.7781\n",
            "Epoch 36/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.5451 - acc: 0.7653 - val_loss: 0.5095 - val_acc: 0.7845\n",
            "Epoch 37/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.5430 - acc: 0.7661 - val_loss: 0.5061 - val_acc: 0.7879\n",
            "Epoch 38/100\n",
            "47520/47520 [==============================] - 3s 69us/step - loss: 0.5448 - acc: 0.7671 - val_loss: 0.5011 - val_acc: 0.7912\n",
            "Epoch 39/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.5397 - acc: 0.7672 - val_loss: 0.5044 - val_acc: 0.7845\n",
            "Epoch 40/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.5376 - acc: 0.7717 - val_loss: 0.4947 - val_acc: 0.7906\n",
            "Epoch 41/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.5329 - acc: 0.7726 - val_loss: 0.5002 - val_acc: 0.7958\n",
            "Epoch 42/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.5327 - acc: 0.7730 - val_loss: 0.4997 - val_acc: 0.7892\n",
            "Epoch 43/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.5293 - acc: 0.7743 - val_loss: 0.4935 - val_acc: 0.7879\n",
            "Epoch 44/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.5314 - acc: 0.7737 - val_loss: 0.5033 - val_acc: 0.7887\n",
            "Epoch 45/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.5283 - acc: 0.7747 - val_loss: 0.4896 - val_acc: 0.7960\n",
            "Epoch 46/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.5245 - acc: 0.7751 - val_loss: 0.4899 - val_acc: 0.7958\n",
            "Epoch 47/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.5239 - acc: 0.7738 - val_loss: 0.4956 - val_acc: 0.7899\n",
            "Epoch 48/100\n",
            "47520/47520 [==============================] - 3s 74us/step - loss: 0.5214 - acc: 0.7767 - val_loss: 0.4890 - val_acc: 0.7938\n",
            "Epoch 49/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5221 - acc: 0.7795 - val_loss: 0.4908 - val_acc: 0.7975\n",
            "Epoch 50/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.5198 - acc: 0.7775 - val_loss: 0.4853 - val_acc: 0.7938\n",
            "Epoch 51/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.5140 - acc: 0.7798 - val_loss: 0.4867 - val_acc: 0.7960\n",
            "Epoch 52/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.5150 - acc: 0.7800 - val_loss: 0.4838 - val_acc: 0.8015\n",
            "Epoch 53/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.5099 - acc: 0.7810 - val_loss: 0.4824 - val_acc: 0.7983\n",
            "Epoch 54/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.5107 - acc: 0.7822 - val_loss: 0.4786 - val_acc: 0.8042\n",
            "Epoch 55/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.5094 - acc: 0.7826 - val_loss: 0.4793 - val_acc: 0.7987\n",
            "Epoch 56/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.5106 - acc: 0.7823 - val_loss: 0.4762 - val_acc: 0.8037\n",
            "Epoch 57/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.5090 - acc: 0.7821 - val_loss: 0.4786 - val_acc: 0.8019\n",
            "Epoch 58/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.5054 - acc: 0.7831 - val_loss: 0.4812 - val_acc: 0.8022\n",
            "Epoch 59/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.5057 - acc: 0.7827 - val_loss: 0.4752 - val_acc: 0.7992\n",
            "Epoch 60/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.5059 - acc: 0.7838 - val_loss: 0.4736 - val_acc: 0.8003\n",
            "Epoch 61/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.5002 - acc: 0.7854 - val_loss: 0.4700 - val_acc: 0.8034\n",
            "Epoch 62/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.5001 - acc: 0.7861 - val_loss: 0.4694 - val_acc: 0.8030\n",
            "Epoch 63/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.4966 - acc: 0.7874 - val_loss: 0.4641 - val_acc: 0.8089\n",
            "Epoch 64/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.5004 - acc: 0.7878 - val_loss: 0.4753 - val_acc: 0.8015\n",
            "Epoch 65/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.5005 - acc: 0.7864 - val_loss: 0.4768 - val_acc: 0.8024\n",
            "Epoch 66/100\n",
            "47520/47520 [==============================] - 4s 83us/step - loss: 0.4950 - acc: 0.7886 - val_loss: 0.4638 - val_acc: 0.8054\n",
            "Epoch 67/100\n",
            "47520/47520 [==============================] - 4s 82us/step - loss: 0.4932 - acc: 0.7887 - val_loss: 0.4726 - val_acc: 0.8039\n",
            "Epoch 68/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.4962 - acc: 0.7887 - val_loss: 0.4655 - val_acc: 0.8051\n",
            "Epoch 69/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4933 - acc: 0.7885 - val_loss: 0.4618 - val_acc: 0.8101\n",
            "Epoch 70/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.4911 - acc: 0.7896 - val_loss: 0.4702 - val_acc: 0.8062\n",
            "Epoch 71/100\n",
            "47520/47520 [==============================] - 3s 69us/step - loss: 0.4927 - acc: 0.7888 - val_loss: 0.4688 - val_acc: 0.8037\n",
            "Epoch 72/100\n",
            "47520/47520 [==============================] - 3s 69us/step - loss: 0.4893 - acc: 0.7915 - val_loss: 0.4620 - val_acc: 0.8056\n",
            "Epoch 73/100\n",
            "47520/47520 [==============================] - 3s 70us/step - loss: 0.4882 - acc: 0.7923 - val_loss: 0.4720 - val_acc: 0.8017\n",
            "Epoch 74/100\n",
            "47520/47520 [==============================] - 3s 74us/step - loss: 0.4892 - acc: 0.7913 - val_loss: 0.4639 - val_acc: 0.8086\n",
            "Epoch 75/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.4871 - acc: 0.7905 - val_loss: 0.4557 - val_acc: 0.8101\n",
            "Epoch 76/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.4842 - acc: 0.7934 - val_loss: 0.4626 - val_acc: 0.8079\n",
            "Epoch 77/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.4861 - acc: 0.7936 - val_loss: 0.4626 - val_acc: 0.8101\n",
            "Epoch 78/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.4870 - acc: 0.7925 - val_loss: 0.4542 - val_acc: 0.8111\n",
            "Epoch 79/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.4841 - acc: 0.7935 - val_loss: 0.4585 - val_acc: 0.8118\n",
            "Epoch 80/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.4858 - acc: 0.7926 - val_loss: 0.4539 - val_acc: 0.8153\n",
            "Epoch 81/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.4798 - acc: 0.7944 - val_loss: 0.4576 - val_acc: 0.8082\n",
            "Epoch 82/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.4792 - acc: 0.7967 - val_loss: 0.4567 - val_acc: 0.8138\n",
            "Epoch 83/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.4797 - acc: 0.7959 - val_loss: 0.4522 - val_acc: 0.8101\n",
            "Epoch 84/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.4797 - acc: 0.7951 - val_loss: 0.4567 - val_acc: 0.8126\n",
            "Epoch 85/100\n",
            "47520/47520 [==============================] - 3s 69us/step - loss: 0.4804 - acc: 0.7954 - val_loss: 0.4567 - val_acc: 0.8094\n",
            "Epoch 86/100\n",
            "47520/47520 [==============================] - 3s 71us/step - loss: 0.4761 - acc: 0.7956 - val_loss: 0.4523 - val_acc: 0.8152\n",
            "Epoch 87/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.4762 - acc: 0.7973 - val_loss: 0.4529 - val_acc: 0.8150\n",
            "Epoch 88/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.4763 - acc: 0.7967 - val_loss: 0.4526 - val_acc: 0.8175\n",
            "Epoch 89/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.4752 - acc: 0.7980 - val_loss: 0.4464 - val_acc: 0.8172\n",
            "Epoch 90/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.4750 - acc: 0.7981 - val_loss: 0.4543 - val_acc: 0.8108\n",
            "Epoch 91/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.4726 - acc: 0.7989 - val_loss: 0.4494 - val_acc: 0.8133\n",
            "Epoch 92/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.4727 - acc: 0.7976 - val_loss: 0.4542 - val_acc: 0.8111\n",
            "Epoch 93/100\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.4710 - acc: 0.7999 - val_loss: 0.4501 - val_acc: 0.8114\n",
            "Epoch 94/100\n",
            "47520/47520 [==============================] - 4s 74us/step - loss: 0.4696 - acc: 0.8007 - val_loss: 0.4506 - val_acc: 0.8120\n",
            "Epoch 95/100\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.4732 - acc: 0.7980 - val_loss: 0.4474 - val_acc: 0.8143\n",
            "Epoch 96/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.4691 - acc: 0.7991 - val_loss: 0.4534 - val_acc: 0.8145\n",
            "Epoch 97/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4700 - acc: 0.8012 - val_loss: 0.4484 - val_acc: 0.8162\n",
            "Epoch 98/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4680 - acc: 0.8001 - val_loss: 0.4533 - val_acc: 0.8109\n",
            "Epoch 99/100\n",
            "47520/47520 [==============================] - 3s 73us/step - loss: 0.4693 - acc: 0.8001 - val_loss: 0.4497 - val_acc: 0.8148\n",
            "Epoch 100/100\n",
            "47520/47520 [==============================] - 3s 72us/step - loss: 0.4679 - acc: 0.7997 - val_loss: 0.4452 - val_acc: 0.8177\n",
            "5940/5940 [==============================] - 0s 17us/step\n",
            "\n",
            "Test accuracy: 82.4%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}